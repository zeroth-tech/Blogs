{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Zeroth Technology blog space","text":"<p>This is where we share our latest thinking, rants, and some ramblings... it's also the archive for everything we've been talking about for years.</p>"},{"location":"2023/10/10/attested-sensors--source-data-synthesis-under-zero-knowledge/","title":"Attested Sensors &amp; Source Data Synthesis Under Zero Knowledge","text":"<p>TLDR; attested sensors can be used to digitally sign source data at the moment it is collected by a IOT device.\u00a0 Blockchain can be used to notarize that signature and apply a tamper proof time stamp.\u00a0 To protect sensitive data from leaving the source, a zero knowledge_ \u00a0circuit can be used to produce a synthesis of the raw data and generate a proof that the synthesis was done correctly.</p> <p>Securing data at the point of origin and delivering data providence throughout its lifecycle is a key component of data integrity.\u00a0 Assuming that data will travel across multiple systems/networks it is important to develop methods that can travel with the data and independently attest to its source attribution and immutability.\u00a0 Note that attesting to the validity of the data in the first place is a separate issue of configuration and physical control.</p> <p>Sensors are hardware that capture evidence or take measurements in the real world and produce a digital output.\u00a0 An example is a digital thermometer that will measure the ambient temperature of its surroundings and provide output data on a periodic basis.\u00a0 Another example is a digital photo that captures an image through a lens and produces an array of colored pixels that can be used to render the image in digital media.\u00a0 Still another example a biometric sensor that can read a human iris or fingerprint and return a digital identifier for that individual.</p> <p>Attested sensors are sensors that have additional hardware which contains a public/private key pair, the public key being a universal identifier for the sensor.\u00a0 When the sensor takes a reading (or a photo for example) the private key is used to sign the digital output of the sensor.\u00a0 This typically includes a hash of the data itself plus an encryption that can only be produced with the private key but can be easily verified by anyone that knows the public key.\u00a0 The hardware must be carefully designed so that there is no option to alter the source data prior to the signature being generated.\u00a0 Coupling the sensor with a network protocol, such as a blockchain, can serve to provide a time stamp for the signature and to store the data hash in an immutable database; this coupling requires internet connectivity for the sensor.</p> <p>Subsequent changes to the data must be handled through careful orchestration of software and permissions to develop a \u2018manifest\u2019 that tracks all alterations in the form of a change log.\u00a0 The work currently being done by CP2A.org is an example of what must be done to ensure edited copies of digital photos are legitimate and attributable.\u00a0 While not as prominent outside of photography, video, and audio, a similar \u2018manifest\u2019 approach could be applied to any sensor reading.</p> <p>Through the function of registering the public key of the sensor to a network (likely the same network used for time stamping and notarization of the data) it is later possible to attribute data back to its source, if the data is to be used as part of a training population for machine learning this is critical component to ensuring data authenticity (and possibly deriving percent attribution which may be used for calculating royalties).</p> <p>Figure 1: Attested sensors with timestamping and signatures </p>","tags":["attested sensors"]},{"location":"2023/10/10/attested-sensors--source-data-synthesis-under-zero-knowledge/#data-capture-with-synthetic-data","title":"Data Capture with Synthetic Data","text":"<p>Some sensors will inherently capture data that is considered sensitive or \u2018Personally Identifiable Information\u2019 (PII) such as biometric scanners \u2013 while the scanner itself may not capture PII, the function of coupling that data with an established identity would.\u00a0 In general, any sensor data may be coupled with non-sensor information such as identity, location, etc. It is that correlation between several data elements that makes the dataset useful; but this dataset also creates privacy concerns.</p> <p>An attested sensor with data synthesis capabilities generates both a raw data set and a synthesized data set for each reading.\u00a0 The sensors synthesis circuit anonymizes the PII from the raw data using a zero knowledge cryptographic circuit which accepts the raw data as input and generates a synthesized data set in addition to a cryptographic proof which is signed by the attested sensor.</p> <p>In this method we are pushing the synthesis of data out to the edges of the network by generating it at the source.\u00a0 This then allows the synthetic data to be aggregated for model training without the risk of sensitive data leakage.\u00a0 The source data can then be encrypted and stored locally in a secure manner, or possibly destroyed.</p> <p>Figure 2: Attested sensors with data synthesis </p> <p>Example 1: Converting raw data sets to synthetic data set  </p> <p>There may be an alternate way of writing the synth circuit then flashing it onto some non-volatile memory in the sensor, it could be part of the attestation process\u2026</p> <p>But then again ZKPs are cool.</p>","tags":["attested sensors"]},{"location":"2023/11/01/digital-twin--product-token/","title":"Digital Twin \u2260 Product Token","text":"<p>TLDR \u2013 Digital twins are for running simulations and optimizing for future events.\u00a0 Product tokens are for transferring claims against an asset (ownership, custody, etc) and recording the state of past events.</p> <p>At the end of this post I pasted a very nice summary from ChatGPT on the definition of digital twin and product tokens - I think the LLM agrees with me.</p> <p>\u2014\u2014</p> <p>A product token is a medium of exchange that records the transfer of a claim on a Real World Asset (in this case a product) between one party and another.\u00a0 Holding a claim on a product token gives the holder certain rights, such as the ability to sign metadata about the product \u2013 over time the metadata + the history of claims transfers builds up a very robust data set about that product and its movements.</p> <p>A digital twin is meant to simulate a Real World Asset in digital form - while it is in fact a digital representation of a unique or of a group of similar products it does not in fact carry all of the product attributes that would typically be required to form a digital twin (while it\u2019s certainly possible to include anything you want in the metadata, it\u2019s unlikely that you would include information about physical properties (i.e., solubility or physical dimensions)).\u00a0 In the classical definition of digital twin these are important parameters that would feed into a simulation.</p> <p>But the two are not completely separate; in defining and monitoring a product token you can learn a lot about the products actual handling in the real world by studying the patterns of movements and data updates which could include typical temperature ranges number of handoffs or amount of time spent in transit - all of this real world data is very valuable in fine tuning your digital twin models and should be used as input into your simulations.</p> <p>However, because there is often a lot of intellectual property associated with a fully structured digital twin it may also make sense to represent that digital twin as a non fungible token. \u00a0This allows the owner of that NFT to claim ownership over the model based on that digital twin; it could allow for verification of results and easier sale or transfer of those digital twins if an organization has hundreds of thousands of digital twins. \u00a0Tokenization in this respect creates a very good way of cataloging an inventory the digital twins themselves and allows fine grained version control over changes to the attributes of each. \u00a0But again, this is not the same as using a product token which is a medium value exchange.</p> <p>Jamey: What is the definition of a digital twin?</p> <p>Chat GPT:</p> <p>A digital twin refers to a virtual or digital representation of a real-world entity, system, process, or object. It's essentially a digital counterpart that mirrors the physical counterpart, allowing for real-time synchronization and interaction between the two. This concept is commonly used in various industries, including manufacturing, engineering, healthcare, urban planning, and more. Here's a more detailed definition: A digital twin is a digital or computer-based representation of a physical entity or system. It encompasses both the physical aspects (geometry, structure, materials) and the functional aspects (behavior, performance, interactions) of the entity. The digital twin is created by collecting and integrating data from various sources, such as sensors, simulations, and historical records. This data is used to create a dynamic, real-time model that reflects the current state and behavior of the physical entity. The digital twin concept offers several benefits, including:</p> <ol> <li>Simulation and Testing: Digital twins enable simulation and testing of real-world scenarios without affecting the physical entity. This is useful in optimizing performance, predicting behavior, and identifying potential issues before they occur.</li> <li>Monitoring and Maintenance: By continuously collecting data from sensors embedded in the physical entity, the digital twin can be used to monitor its condition, performance, and health. This data can inform maintenance schedules and prevent downtime.</li> <li>Analysis and Optimization: Digital twins provide a platform for analyzing data and optimizing processes. They can be used to identify inefficiencies, improve resource allocation, and enhance overall performance.</li> <li>Predictive Capabilities: With historical and real-time data, digital twins can make predictions about future behavior and performance, aiding in decision-making and planning.</li> <li>Collaboration: Digital twins facilitate collaboration between various stakeholders, such as designers, engineers, and operators. They provide a shared platform for understanding and working with complex systems.</li> </ol> <p>Examples of digital twins range from individual products like cars or turbines to larger systems like smart cities or entire manufacturing plants. As technology advances, digital twins are becoming more sophisticated, incorporating elements of artificial intelligence, machine learning, and IoT (Internet of Things) connectivity to enhance their accuracy and capabilities.</p> <p>Jamey: What is the definition of a tokenized product on a blockchain</p> <p>Chat GPT:</p> <p>A tokenized product on a blockchain refers to a digital representation of a physical or digital item that has been converted into a unique digital token on a blockchain network. These tokens are often used to represent ownership, authenticity, or some form of value associated with the underlying product. Tokenization involves assigning a unique identifier or token to a specific product and recording that information on a blockchain. Here's a breakdown of the key concepts:</p> <ol> <li>Tokenization: Tokenization is the process of creating a digital token that represents a real-world asset, item, or product. In the context of products, tokenization involves converting the attributes and ownership of a physical or digital item into a digital token that can be tracked, traded, and verified on a blockchain.</li> <li>Product: The term \"product\" in this context can refer to a wide range of items, including physical goods (e.g., luxury items, collectibles, real estate) or digital assets (e.g., software licenses, digital art, music).</li> <li>Blockchain: A blockchain is a decentralized and immutable digital ledger that records transactions in a secure and transparent manner. Each transaction is grouped into a block and linked in chronological order, creating a chain of blocks. This technology provides security, transparency, and trust in the recorded data.</li> <li>Token: In the context of blockchain, a token is a unit of digital value or representation that can be stored and transferred on the blockchain. Tokens can have various functions, such as representing ownership, granting access to certain resources, or serving as a medium of exchange.</li> </ol> <p>Tokenized products offer several benefits, including:</p> <ul> <li>Ownership and Provenance: Tokenization provides a transparent and verifiable record of ownership and the history of the product. This can be especially useful for tracking the authenticity and provenance of luxury goods, collectibles, and art.</li> <li>Fractional Ownership: Tokenization allows for fractional ownership, enabling multiple individuals to own a share of a high-value item. This can increase accessibility to assets that might otherwise be out of reach.</li> <li>Liquidity: Tokens representing products can be traded on blockchain-based marketplaces, increasing the liquidity of traditionally illiquid assets.</li> <li>Interoperability: Tokenized products can be integrated with other blockchain-based systems and services, enhancing interoperability and creating new opportunities for innovation.</li> </ul> <p>It's important to note that while tokenization offers various advantages, it also comes with regulatory considerations and challenges related to ensuring the accuracy of the physical asset's representation, verifying authenticity, and complying with legal requirements in different jurisdictions.</p>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/","title":"Does Knowing Make You Accountable?","text":"<p>In my recent post about supply chain a friend replied in the comments that \u201cI\u2019ve come to the realization that big business doesn\u2019t want that level of transparency, because transparency = accountability.\u201d.\u00a0 To which my quick counter reply was \u201cBeing able to keep transactions confidential is key, but even this creates accountability - so we need an incentive that can outweigh that risk\u00a0\u201d.\u00a0 I then suggested that this would be a topic for my next post, so here it is </p> <p>Does knowing make you accountable?\u00a0 What are the real motivations for supply chain transparency.</p> <p>I worked on a project early in my career where we were writing an algorithm to calculate the rate at which product complaints were received over a three-week rolling window so that we might predict if we would hit a certain threshold in week four; \u00a0which would trigger an investigation.\u00a0 Because an investigation required resources, this would help us better plan \u2013 we were going to have better, more timely inspections and save the organization millions of dollars due to more efficient resource planning.\u00a0 As a side note, we didn\u2019t call this AI at the time \u2013 it was just statistical trend analysis\u2026 The math worked, but the program was never implemented.\u00a0 Leadership realized that once they had knowledge that an investigation should be done they had an obligation to do it, and this opened them up to additional liabilities and potentially raised the bar on all kinds of predictive actions they should take.</p> <p>At the time I didn\u2019t quite agree that sometimes it makes better business sense to \u201cnot know something\u201d.\u00a0 15 years later I still don\u2019t quite agree, but I understand the position\u2026 everything is complicated.\u00a0 In my opinion to not do something because it could make you accountable for not doing enough, means you just aren\u2019t motivated to do the thing in the first place \u2013 even if you know it\u2019s the right thing to do. \u00a0And if it is the right thing to do eventually it will be done (one way or another).\u00a0 So, while the statement that \u201cBusinesses don\u2019t want traceability because traceability = accountability\u201d is generally correct (especially in a very narrow time frame) \u2013 but it\u2019s also an excuse to avoid doing the work today.\u00a0 Accountability is a risk, if you are not prepared to stand behind your actions, but it you are doing the work accountability can be a protection.</p> <p>To fully round out this perspective we must also consider \u201cto whom are you accountable\u201d.\u00a0 If the entire world is watching then that is a pretty high bar.\u00a0 But if it is your business partners, customers, and regulators then you can set expectations appropriately.\u00a0 This is why confidential transactions are \u201ckey\u201d and when we talk about traceability it\u2019s important to limit that visibility into only those things you should have access to see \u2013 IMO determining the \u201cshould\u201d is the role of claims as discussed in my previous post.</p> <p>But this really boils down to \u201chow do you get people to do the work\u201d to participate in a network that provides supply chain visibility to those who should have it.\u00a0 Its\u2019 worth reflecting here on the relationship between motivation (an internal driver) and incentive (an external driver).\u00a0 I believe incentives are a catalyst to get people (or enterprises) to act on latent motivations, and that not everyone will react to the same incentives.\u00a0 Conversely, if there is no motivation, then most incentives will be short lived.\u00a0 So, in order to create supply chain visibility, we must make 3 basic assumptions:</p> <ol> <li>There are at least some participants in the supply chain who are motivated to do this. These are likely the endpoints \u2013 producers and consumers.</li> <li>Not everyone in the supply chain will respond to the same incentives.</li> <li>The adoption of such a solution will be gradual \u2013 there will be a (hopefully not-to-long) period of time where not all supply chain participants will be motivated to participate.</li> </ol> <p>There is also one more important component here.\u00a0 Compliance is not a motivator; at best it should be one of several incentives that get enterprises to take action.\u00a0 I know this is an idealistic statement, but I am a firm believer that if we want to build a sustainable solution we cannot lead with the stick \u2013 supply chain visibility must provide business value first, regulations are the guardrails, and compliance is a byproduct.</p> <p>Incentives:</p> <p>In my experience incentives always fall into two categories:</p> <ol> <li>Reduce Cost</li> <li>Increase Revenue With the latter being the more significant driver, but reducing costs is usually the lower hanging fruit and is typically easier to quantify in terms of value. Here are just a few incentives that have come up through the supply chain projects I\u2019ve had the privilege to be a part of:</li> </ol>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/#reducing-cost","title":"Reducing Cost","text":"<ul> <li>Reduction of friction: Anywhere that there are delays in moving products or where processes rely on some form of a reconciliation to \"true up\" after a certain period of time is an area of friction. I love the word \"friction\" in this context - to me is sums all the stuff that doesn't make sense but we do anyway. Supply chain visibility and a common ledger for settling inventory eliminates this kind of friction.</li> <li>Better planning: Imagine the impact on planning cycles if a manufacturer were able to \"see\" their products being consumed in near real time. Or how we could reduce safety stock requirements when we could see inventory flow accurately. One of my favorite use cases is inventory redistribution for products with a shelf life - if we know something is about to expire, lets send a replacement and get the near expired one to someone that will use it.</li> <li>Directed Flow: Dynamically routing products based on attributes and inputs from some trusted oracle. Admittedly this use case is a little further out there, but the same way that DeFi created programable money with smart contracts we will see programable inventory.</li> </ul>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/#increasing-revenue","title":"Increasing Revenue","text":"<ul> <li>New Markets: With better product linage and the ability to \"prove\" compliance with local laws companies with true supply chain visibility will be better positioned to win new country level tenders and bring products to new markets faster.</li> <li>Better Payments and Discounts: The next logical step from trusted supply chain visibility will be a better system for payments triggered by real world events (ironically this is the antithesis of procurement based supply chain visibility...). In the pharma industry the world of rebates and chargebacks has long been targeted as an area that blockchain will disrupt. one could argue that payments and discounts are a cost savings, but I believe if done correctly these actually lead to unlocking additional markets and customers.</li> <li>Collaborations and Joint Ventures: Being able to pair your products with something that complements them allows you to specialize. Trusted supply chain visibility and a shared ledger of inventory settlements makes it WAY easier to collaborate and enforce cooperative agreements.</li> </ul> <p>I'm sure I could go on and on, but this is a good list to get started.  The point is that forward and backward traceability (trusted supply chain visibility) unlocks business value for those with the right perspective.  For those without that perspective it will likely disrupt their business.  So coming back to the original question of \"Does knowing make you accountable?\" I think the answer is \"no\"; you were always accountable, but you just didn't know it.  I'm of the opinion that more (trusted) information is always better, and that knowing as much (or more) than your competition is sound business strategy.  While there is a risk that comes with accountability, the from the right lens of incentives the benefits will far outweigh those risks.</p>","tags":["tokenization","supply chain"]},{"location":"2024/05/10/interobjectivity-/","title":"Interobjectivity... ?","text":"<p>I love a good buzz word, especially one that sounds smart; could this be a new narrative that helps us express our intents in a digital world?  In exploring where this came from it helped me crystalize my understating of the difference between rules and laws in autonomous virtual worlds.  Even though that sounds like a stretch, this is gong to effect the way we do things in the real world sooner rather than later.</p> <p>Let me share with you a fun little rabbit hole I jumped into the other weekend around \"Interobjectivity\".  This peaked my interest after hearing the word thrown around on a couple of podcasts and me realizing that my assumed definition based on the context clues of how it was used was conflicted.  Because \"interobjective\" kind of sounds like \"interoperable\" I had assumed it meant some sort of common ground where multiple objectives could be met with minimal trade-offs.  Turns out that is not correct, but it's not completely wrong either.  Into the hole we go...</p> <p>First of all the word interobjectivity is a very rarely used outside of social sciences, and there seem to be several formal definitions of it.  From my limited research, the word first popped up in 1996 by the French philosopher Bruno Latour\u00a0 His paper \u201cOn Interobjectivity\u201d<sup>1</sup>.  Which is a fascinating foray into the mating habits of baboons and a drawn out example of him talking through a metal grate in order to buy postage stamps.  But within the 20 pages of philosophical prose there is a theorem there - humans (unlike other species) have a special kind of relationship with objects, we can use them to execute our intent even when we are not around.  Latour uses an example of a shepherd that builds a fence.  Previously the shepherd would need to constantly roam around and herd his sheep to keep them in position, now instead of the shepherd needing to interact with the sheep directly the fence does it for him and he can take a nap.   The relationship between the shepherd and the fence is interobjective, because of the fence's permanence it allows the shepherd to extend his influence on the actions of others (in this case the sheep) even while he is asleep, or (depending on how well he built the fence) long after he is gone.</p> <p>Oddly enough, though the paper is titled \"On Interobjectivity\" Latour does not actually define the term (or perhaps he did in the original French)... Later a grad student contributing to the \"Human Geography Knowledge Base\" gave interobjectivity the definition of: \"the common world of experience and meaning which is shared by groups of people\u00a0and\u00a0objects. \"<sup>2</sup></p> <p>There is a 2016 book \"On the Existance of Digital Objects\"<sup>3</sup> where Bernard Stiegler contributed a chaper \"The Time of Technical Systems\" where he uses the word interobjectivity in the context of information systems - I've ordered the book and added it to my summer reading list. The most official publicly available definition I could find was in the Encyclopedia of Critical Psychology and states that \"The concept of interobjectivity has been introduced and developed in the social sciences to account for the non-conscious engagement in the course of social interaction that occurs within a social field that is phenomenally objective for subjects and that includes interactions with objects.\" <sup>4</sup> A full write up of this definition can be found here. This probably makes sense to psychologists, but was a little murky for me.  Though it is pretty clear that the root of the word (objective) is referring to being able to sense something physical.</p> <p>Unfortunately I don't think the existing definition extends very well into how we've been trying to use the word in the crypto space - which is mostly digital.  When I think of Latour's paper the part that sticks with me was the ability to use objects to express intent and the lasting effect that it has on others to convey that intent.  The shepherd intended for the sheep to stay corralled, in his mind he made a rule stating \u201cthou sheep shall not pass\u201d and he built a fence to enforce that rule.  Therefore my layman's definition of interobjectivity would be \u201cthe way a human can use a non-human object to express their intent to other humans\".  Or given the state of technology today perhaps a more apt definition is \"the way an intelligence can use a non-intelligent object to express their intent to other intelligences\".</p> <p>This flows nicely from the definition of intersubjectivity which is a word and has been well defined and discussed in many sociology journals. The short definition is \"the intersection or relation between the cognitive perspectives of individuals\".  Interobjectivity places an object in between the individuals.</p> <p>Info</p> <p>Recently intersubjectivity made an appearance in crypto circles in EigenLayer's white paper describing the EIGEN token as an \u201cintersubjective work token\u201d basically saying that you can post some EIGEN as a bond to do some work that isn\u2019t particularly easy to measure. Later, if two or more reasonable people agree that you didn\u2019t do said work, you can lose your bond.  Thus widening the aperture of what a work token can be used for the current objective measures.</p> <p>This really has nothing to do with what we are discussing here, but it is an interesting reference.</p> <p>To stretch this into the digital we need to take the leap that objects are not just physical things.  In a pure coincidence of words, developers have been using \"object-oriented\" programming languages since the 60's for their ability to encapsulate code and extend functionality. So instead of the shepherd building a physical fence, a game developer could create an instance of an object class of fence and give it attributes such as height and length.  When a digital sheep encountered such an object it would be programmed to take some action such as turn around.  Whenever someone is playing that game the developer's intent conveys by the attributes of the object they define and how that object interacts with other objects within the game.</p> <p>But now the slope gets slippery... Intent is a difficult thing, if someone needs to interpret your intent it is by nature subjective and we are trying to build an interobjective world.  In the physical world intent often differs from the outcome, or causes unintended outcomes.  While the fence may keep the sheep in, it also might keep others out. In software development we do our best to be explicit with expressing intent in the form well defined requirements, but even this gets tricky because of unintended outcomes or the vast array of interactions with other objects.  The best way to avoid slipping down the slope is to use a language that is explicit and universal where all possible outcomes are deterministic - such as math.</p> <p>Autonomous Worlds:</p> <p>Here we are going to take a bit of a shift into digital or virtual worlds.  I'm not talking about the metaverse, I'm referring to worlds as an all encompassing container.  Back in 2022 ludens started writing about \"Autonomous Worlds\" as fully on-chain rulesets in the 0xparc.org blog and later went on to build entire frameworks and toolsets for creating virtual worlds on-chain with Lattice.  In the spirit of standing on shoulders - many of the concepts I discuss below have been more fully fleshed out by the Lattice team (I'm a big fan of their work).</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#expressing-intent-rules-vs-laws","title":"Expressing Intent: Rules vs. Laws","text":"<p>In the physical world the transition from expressing intent to getting others to take a specific action is a matter of how well the object is designed and applied.  If the shepherd used a piece of string to corral the sheep, it is a weak form of expression, if he built a 10 foot wall it\u2019s a strong form.  But even the strongest wall can be breached if you had really determined sheep\u2026 though this is drifting away from our interobjective rant a bit, it\u2019s important to note that we don\u2019t always need a physical or digital object to express intent, we can simply create a rule and enforce it through some sort of social construct or norm.  In fact we often need to do this to account for weak forms of interobjective expression when strong forms are not practical.  The shepherd could hang a sign on the string fence to let the sheep know we don\u2019t want them to cross\u2026 and we could decide to strengthen the rule by making it a law and apply stiff penalties if violated. But alas, rules and laws are made to be broken (and most sheep can't read)...</p> <p>But not all laws can be broken\u2026 we live in a universe that is bound by physical or natural laws.  These are concise descriptions of natural phenomena that have existed long before us humans and will likely be intact long after we are gone.  Through the scientific process we have observed and tested these descriptions and to the best of our knowledge they cannot be broken.  These include constants like the speed of light, or the gravitational constant, but concise descriptions are often formulas that relate these constants to other things we can measure like \\(E=MC^2\\).  Because we don't know a reality where these constants don't exist we take them for granted and in many cases don't even recognize them - but they are always there.  The existence of natural laws shape our reality, they become our basis for establishing truth and there is an inherent belief that they will continue to operate in the same way in the future. My kid tried to ignore gravity once - that didn't end well... I doubt he's going to try that again.</p> <p>I find it interesting that in the study of physics the most concise way to describe a natural phenomena is through the use of a mathematical formula.  Math is the universal language and can be both explicit and deterministic.  There is a popular saying in crypto that \"code is law\" and as catchy as this saying is, if we want to apply it to natural laws as opposed to social norms we need to add a few conditions.</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#establishing-natural-laws-in-an-interobjective-autonomous-world","title":"Establishing Natural Laws in an Interobjective Autonomous World","text":"<p>I was playing Minecraft with my son the other day and there were a few blocks of land suspended in the air.  It felt weird because I'm used to real-world physical laws, but that is how the developers designed the game - only certain blocks fall.  Accepting that reality is a kind of informal player contract that I agree to when playing the game.  Part of the fun is experimenting and discovering just what you can and can't do.  But it's only fun because Minecraft's physics are deterministic - once you understand the parameters they operate the same way all the time.  Sure there is randomness in how the worlds are laid out, but there is no randomness in how each block interoperates with each other.  If sometimes blocks fell up, and sometimes blocks fell down or some just randomly vanished, the game play would be completely different - it would be very frustrating...</p> <p>Important</p> <p>Interobjectivity relies on natural laws in a digital world, and those laws must be deterministic</p> <p>Deterministic natural laws set the foundation for how the digital world operates, they form the basis for that informal player contract which is elected into each time you play.  Their consistency allows players to make predictions about how their actions will effect the world and will effect other players or will allow a player to use an object to express their intent to other players.  If the world is to have some permanence (i.e., your actions effect others even when you are not playing) it is reliance on the natural laws that create an interobjective experience.</p> <p>Being deterministic also implies that things are rigid.  The art of designing a good game rests in finding the right balance between deterministic laws and rules which players opt into.</p> <p>Important</p> <p>Deterministic laws should be simple and basic - if done well they aren't even noticeable.</p> <p>What's not fun in Minecraft is when you've been grinding away for hours looking for diamonds and your kid turns on creative mode when your not around just to fills up his inventory.  The ability to turn on creative mode (unlimited inventory) or peaceful mode (you can't get killed) can add a lot to the experience, but when one or just a few people have \"God-Mode\" powers it really disincentives other players.  Just like in real life, changing the rules should not be arbitrary and requires some form of governance.</p> <p>Important</p> <p>Rules and laws are different.  Rules can be changed and require governance; laws are immutable.</p> <p>But both the law and the rules are just code running on some computer.  One cannot claim to have immutable code if they control the hardware and the technology stack - if the game is contained within a world that runs on a computer (which might be running multiple worlds), then whoever can turn off the computer has God-Mode over the universe!  Right now the majority of all digital worlds run on servers that are controlled by just a handful of enterprises.  It's easy to \"trust\" them when the stakes are low but they are most certainly looking out for their own interests (and the interests of their share holders).  Chris Dixon points this out in his fantastic book \"Read, Write, Own\". <sup>5</sup> He says the only way to prevent this is through protocol networks, or more specifically by thinking of blockchains as virtual world computers (not to be confused with individual computers running virtual worlds).  The \"protocols\" enforce the natural laws of the network across a vast array of computers (I.e., nodes in a decentralized network), it becomes very, very hard to break them.  There is no Universal-God-Mode only coordination among node operators.</p> <p>The other point that Dixon repeatedly makes (and I think is the most important aspect of blockchain in the context of interobjective autonomous worlds) is the unique ability of blockchain to make grantees about future execution of rules - only in a decentralized blockchain can you know that the laws and rules that exist today today will run the same way tomorrow - which makes them uniquely suited for enforcing natural laws.  In the context of interobjectivity this cannot be understated.  In the physical world I can build a fence to keep the sheep in, and I know that because the sheep can't defy gravity they are going to be bound by the fence.  In my digital world I need a guarantee that gravity is going to remain constant if I'm to use the same concept of an object (the fence) to control the sheep.</p> <p>And it's not just constants (which are the parameters) it's the actual execution of the math.  Different computers do math differently - mostly because of the difficulty in handling floating-point precision and various short cuts that computers can take to perform frequently used functions.  If you are looking for another rabbits hole to jump into, this old reddit thread is pretty fascinating.  Most of the time these differences are minute, but they can stack up.  Well designed blockchains execute on a wide array of hardware and software clients so there are always multiple nodes \"checking the math\".</p> <p>Interobjective Autonomous World needs 3 things:</p> <ol> <li> <p>A set of deterministic natural laws enforced via protocols</p> </li> <li> <p>A set of rules that can be opted into by players and maintained through governance</p> </li> <li> <p>Guarantees about the consistency in future execution of 1 &amp; 2</p> </li> </ol>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#a-physical-world-bound-by-virtual-laws","title":"A Physical World Bound by Virtual Laws'...'","text":"<p>I usually write about supply chains and enterprise blockchain use cases.  So far this rant has been about obscure sociology papers and video games\u2026 is there a connection?</p> <p>Yeah, absolutely.  We shouldn't be surprised that gamers are leading the charge in pushing the role of networks for global coordination.  This could be another long post on it's own, but I think it comes down to the fact that we willingly suspend our concept of reality and accept the laws and rules defined by the game designers when we choose to play a game.   In the \"real-world\" game of supply chain (for example) it's not that easy.  There was no mastermind in designing global trade routes, there is no God-mode that can override the laws of nature, we are lucky if we can agree upon a governance structure for enforcing our man-made rules!  The reality is that enterprises exist in silos today and are primarily responsible for keeping their own house in order.  In some ways enterprise systems are like individual worlds that are trying to coordinate with other worlds, but there are no deterministic protocol level rules that give them a foundation to build on.  So every \"object\" they create to express their intent (a contract, a data standard, a packet of business logic) needs to be completely replicated in the other worlds.  And there is no guarantee that those other worlds will A) do it right in the first place, or B) continue to do it right in the future - so we have to jump through all kinds of hoops to \"prove\" our systems to each other.  </p> <p>supplier: \"I built a fence that keeps the sheep in.  It's 4.54 ft tall and surrounds 1.26 acres\"</p> <p>customer: \"ok, I'll build the exact same fence, but I can only go to 1 decimal place so its 4.5 ft tall and 1.3 acres\"</p> <p>supplier: \"hmm... well that's not exactly the same.  Lets just keep checking to see if the sheep can get out\"</p> <p>customer:  \"sounds good, I'll go hire an auditor\"</p> <p>Think about how different global supply chain would be if for certain business transactions the settlement was occurring in an interobjective autonomous world?  The areas of friction could be eliminated because the double checking that needed to occur between systems (or that requires an intermediary) could be eliminated.</p> <p>Just to be clear - I am not proposing a global platform (just the thought of that gives me an upset stomach).  For all the reasons stated above this does not work if there is a God-Mode.  What we is a way to break down business transactions into a series of deterministic natural laws and use a blockchain (or multiple blockchains) to execute those laws the same way for everyone for all time.  Participants can opt-in or opt-out for higher level \"rules\" which help manage their specific supply chains, but they can't break the laws.</p> <p>Tip</p> <p>Example Supply Chain Law: If I have custody over 10 units of inventory I can only transfer custody of 10, or fewer, units of inventory. Example Supply Chain Rule: If I want to destroy a unit I must have both custody and ownership of the unit.</p> <p>Both rules and laws are deterministic and can be described mathematically.  This is the subject of our supply chain work over at Zeroth.Technology.  We are working on a method to allow for confidential settlements with embedded rights for physical products leveraging protocol networks.</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#tying-it-all-back-together","title":"Tying It All Back Together","text":"<p>The notion of interobjectivity is exciting in the context of autonomous worlds, but it requires some work. In the physical world we use objects to express our intent with various degrees of permanence, but even these need the support of societal norms and governance to be effective in the long run.  Applying this to the digital realm in a trustable, sustainable way challenges the underpinnings of existing networks and systems but is completely doable with well designed  blockchains that can execute deterministic laws at the protocol level and allow for better governance around participation rules..  </p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#references","title":"References","text":"<ol> <li> <p>Latour, B. (1996), On interobjectivity. In: Mind, culture and activity. Volume 3, number 4. Retrieved from Scribd.com, Friday May 31st 2024. https://www.scribd.com/document/277023110/Bruno-Latour-On-interobjectivity-Mind-Culture-and-Activity-1996 \u21a9</p> </li> <li> <p>Hui, Y. (2016). The Time of Technical Systems. In On the Existence of Digital Objects (pp. 151-186). University of Minnesota Press. https://doi.org/10.5749/minnesota/9780816698905.003.0005\u00a0\u21a9</p> </li> <li> <p>Human Geography Knowledge Base. (2012). Interobjectivity. Retrieved June 10, 2024, from https://geography.ruhosting.nl/geography/index.php?title=Interobjectivity\u00a0\u21a9</p> </li> <li> <p>Sammut, G., Moghaddam, F. (2014). Interobjectivity. In: Teo, T. (eds) Encyclopedia of Critical Psychology. Springer, New York, NY. https://doi.org/10.1007/978-1-4614-5583-7_158 \u21a9</p> </li> <li> <p>Dixon, Chris.\u00a0Read Write\u00a0Own: Building\u00a0the Next Era\u00a0of the Internet_. Random House\u00a0Publishing Group, 2024.\u00a0\u21a9</p> </li> </ol>","tags":["autonomous worlds","supply chain"]},{"location":"2023/08/20/product-oriented-data-structure/","title":"Product-Oriented Data Structure","text":"<p>Where is most of the effort going to be for enterprises to get on board with product tokenization for track &amp; trace?\u00a0 It\u2019s not the blockchain tech - while that part is cool, if we go with the assumption that any decentralized system worth building on needs to be open source and multi-purpose, it\u2019s not going to be the enterprise\u2019s role to develop or operate that network.\u00a0 It is however the enterprise\u2019s role to lay out clear business requirements that can be used to build modular software (building blocks) to achieve lofty traceability-related use cases.</p> <p>No, the hard work for the enterprise is going to be in organizing their data and mapping their supply chain events into universally understood buckets.\u00a0 The good news here is that this isn\u2019t a new issue, standards for communicating between supply chain systems have been around for decades, the EPCIS standard and the acceding Core Business Vocabulary by GS1 is probably the most widely leveraged and for the most parts fits the needs.\u00a0 My personal experience having implemented blockchain for supply chain solutions for over 5 years now, is that 80% of the effort is in extracting, transforming and loading (ETL) data on an on-going basis. The \u2018on-going\u2019 part also implies that enterprises understand the event triggers that start the ETL process that would feed the supply chain data into such a solution.</p> <p>You might ask: Why is this so hard?\u00a0 Haven\u2019t enterprises been investing in better data structure for years?\u00a0 What about my $20M data lake?\u00a0\u00a0</p> <p>My reply: Have you ever tried to pour a cup of water into a lake and then later scoop out that same cup of water?\u00a0 Modern data architectures are designed for aggregation, not for granularity.</p> <p>I blame it on the ERPs.\u00a0 Enterprises have done everything they can to optimize their business processes, which often starts with the introduction of Enterprise Resource Planning systems (ERPs) and this focus on efficient process has generated a \u2018process-oriented data structure\u2019. It\u2019s easy to see why. If you map out your manufacturing and distribution processes for just one plant, you can probably draw some pretty clear demarcations around which system is governing which part of the process - you likely have interfaces where one system hands off to the next.\u00a0 If you look at how the data is structured within that process, it is generally geared towards records and logs - the data describes what happened over this time period within this process.</p> <p>If you did the same exercise for multiple plants and distribution processes for the same time period and stacked the maps on top of each other (in a 1980s overhead projector style), you might even see how well they line up - but each individual flow is not necessarily connected to each other - you can actually see the data silos forming.\u00a0 This is ultimately the result of process-oriented data structures.\u00a0 Breaking it down properly to a more granular level is a complex matter of mapping and standardization.\u00a0 An easier solution is to just dump all the data into one bucket - or if you have enough of it, into one large lake\u2026\u00a0 But now extend that flow out past the manufacturing and distribution that you control - when the data \u201cbelongs\u201d to an entirely different set of systems (I.e., that of your customer, or your customers\u2019 customer) the idea of capturing all of their data in your data lake is untenable.</p>","tags":["tokenization","supply chain"]},{"location":"2023/08/20/product-oriented-data-structure/#enter-the-product-world","title":"Enter the product world","text":"<p>In the real-world products behave differently than processes in the sense that if you follow a product \u201ccradle to grave\u201d you cross over several (sometimes many) different processes, and very rarely do any two products follow the same path.\u00a0 I have written a lot about product tokens and the role that public blockchains will play in detangling this mess - that is a more in-depth discussion.\u00a0 For the purposes of this post, I just want to focus on the product token as the common thread that connects data across an ecosystem as the product moves through its lifecycle.\u00a0 If we wanted to be a little technical, you could think of the product token as the digital ID which represents a single product, and is the index against which all various databases containing information about that product can be queried.\u00a0 Keeping consistent with our GS1 example above it\u2019s easy to picture the product token ID as a Serialized Global Trade Identification Number (SGTIN).</p> <p>Of course, it\u2019s not as easy as just reindexing databases (as if that were even feasible); we need a mechanism to serve up data about any given product token from any database where it is stored and because this is trustless blockchain land, we want a mechanism to prove that the data has remained unchanged since the time the supply chain event occurred.\u00a0 Let\u2019s unpack that a bit\u2026</p> <p>For starters we will refer to product data as metadata - and this is simply to imply that there is a dataset separate of any other information that enshrines everything a process-oriented data structure knows about a given product.\u00a0 Depending on your use case there are endless \u201cstandards\u201d of what data should be included and what format it needs to take., IMHO this will never end, there will always be some new piece of information that is critical to know about a product so that something different can happen somewhere way down the supply chain.\u00a0 The point is metadata will not be statically defined, and enterprises will need data governance for product data in and of itself.</p> <p>The second point is product metadata will not all live in one place. - As a product goes from production to consumption it crosses many governing systems, each of those systems may add some information to the product metadata, but it\u2019s very unlikely that they will ever write that data back to the initial repository.\u00a0 I like to picture this as a trail of breadcrumbs that a product leaves, but the product itself is the key to linking all the breadcrumbs.</p> <p>It\u2019s worth noting that I do believe decentralized data storage could be a very good answer to keeping product metadata aggregated and accessible when needed - but I don\u2019t think we can assume everyone will be willing to do that. - it should always be the right of the data owner to deny access to the product data, and while there may be consequences to enacting that right, it should be available\u2026</p> <p>Whether each ecosystem partner maintains their own product metadata or you choose to use a decentralized solution, the enterprise will still need to produce the product metadata in the first place.\u00a0 It has become common place for the metadata to be stored in a .json blob.\u00a0 For those non-technical readers, this is just a structured data set consisting of a key value pair.\u00a0 The key being the name of the data element (think field name or column header) and the value being the actual data.\u00a0 Conveying both the key and the value every time may seem like additional overhead, but in the long run it solves sooo many issues by letting the metadata live independently of a fixed the database schema.</p> <p>The other nice thing about using a .json blob is that we can create a hash of it (a one-way encryption, or a digital fingerprint) that can later be used to prove the metadata remained unchanged since the time it was posted.\u00a0 I can also have a portion of the .json blob encrypted using a specific public key (say that of a government regulator) who could at a later date decrypt that section and see some information about the product that is available only to them, but have the confidence that it was there all along and has remained unaltered.</p> <p>Just to reiterate the point.\u00a0 A Product-Oriented Data Structure consists of a universal product identifier for a specific unique product (or batch of identical products) - this is the token ID - and a series of .json blobs containing data specific to that product and likely stored in several different databases managed by different members of the ecosystem.\u00a0 Leveraging the product token as the access mechanism for that data is a process that I call Token Claim Derived Authority (TCDA) which you can read all about in some of my other posts. </p>","tags":["tokenization","supply chain"]},{"location":"2023/08/20/product-oriented-data-structure/#so-what-does-this-mean-for-the-enterprise","title":"So what does this mean for the enterprise?","text":"<p>Over time I\u2019m hopeful that ERPs will begin making it easy to generate a product-oriented data structure for individual products (some claim to do this today, but I think there is still work to do).\u00a0 In the near-term this is the world of Extract Transform &amp; Load (ETL) scripts, and these can be very tricky\u2026. If you happen to be doing a supply chain transformation, and even if you don\u2019t buy into any of the product tokenization stuff (yet) it still cannot hurt you to develop the product-oriented data structure and the governance processes needed to support it.\u00a0 Worst case scenario is you begin using this method to archive your product data or transfer it to a buyer when you sell off the part of your business responsible for that product. \u00a0 Best case scenario, is this becomes second nature to how you collect and store data, tying the data integrity of the product metadata directly to the physical product itself - in this way you begin to limit your liability of providing data on behalf of others in the ecosystem.</p> A short tangent\u2026 <p>There is an interesting aspect that comes up when we assume that not everyone will play nicely\u2026. Let\u2019s look at a couple of cases:</p> <ol> <li> <p>You buy materials from a supplier, but they don\u2019t provide you product metadata, or the metadata they do provide is incomplete (I.e., you trust what they give you, but they don\u2019t give you everything you need for your use case).\u00a0 Here, you would need to fill in the gaps. We already do this all the time today, but we tend to do it at an aggregate-level, and we do it infrequently.\u00a0 I would propose that enterprises develop a product data profile of what they would expect to receive and in which format (not a bad idea to share this with the supplier\u2026).\u00a0 Once enough data has been collected from different suppliers, then you can develop estimation formulas (this is one possible benefit of AI) that fill in the gaps for each new product you receive.\u00a0 It\u2019s really important in this scenario that those estimated data elements be flagged as such.</p> </li> <li> <p>You buy materials from a supplier, but they include fake data in their product metadata.\u00a0 Let\u2019s assume they do this maliciously because accidental mistakes should be easier to catch.\u00a0 This is where the blockchain can hold someone accountable - if the supplier signs that metadata they are in effect attesting to its accuracy.\u00a0 You as the buyer can definitively point to the source of the data as not being your own (let the legal updates to your purchasing agreements ensue\u2026).\u00a0 Further, if you have the estimation models from point 1 you can scan the metadata from each incoming product token and look for any outliers (this is why the accidents will be easier to catch).\u00a0 If over time things look too good to be true, they probably are\u2026. To that point it\u2019s not a bad idea to put something dynamic in the metadata so that your can detect copy/paste records.</p> </li> </ol> <p>Special thanks to my colleague Akhil Patil for some constructive feedback and many grammatical corrections  </p>","tags":["tokenization","supply chain"]},{"location":"2023/10/18/product-token-metadata-profiling/","title":"Product Token Metadata Profiling","text":"<p>Often the topic of AI regarding blockchain comes up, often the topic is misguided\u2026</p> <p>A blockchain is not a learning system, it is not intelligent (smart contracts aren\u2019t even that smart\u2026).  It\u2019s more akin to a data historian, a network that maintains a state, and a record of all previous states.  It does not adapt on its own or self optimize - if it did it would undermine the trusted execution.</p> <p>There are a couple of places where AI and blockchain do meet, I\u2019m sure I\u2019m missing a few here, but these are top-of-mind:</p> <p>1) Attribution: as far as we know today the robots are not smart enough to crack our private key encryption and sign fraudulent transactions - so taking a piece of data and notarizing it on chain is a good way of attributing that data to a source.  If we also mix in a verified credential, the wallet that signed the data can carry some certification or reputation.  When training machine learning models this level of attribution will be increasingly important to avoid bias or to have a \u201cprovably legitimate\u201d training data set. 2) Data profiling:  personally I think this one is much more interesting.  We are envisioning a world where product tokens are accompanied by multiple .json blobs of data specific to that token.  The rest of this post builds off of this idea.</p> <p>If I am manufacturing a product - say a bottle of water - it\u2019s likely that I will have several different sets of metadata about that product each for different use cases.  For example I may have metadata about the source of the water.  I may have metadata about the global trade information. I may have metadata about the carbon footprint (though that should be captured in an allocated carbon token), I might even have metadata on the label or instructions for use (imagine that\u2026 how to use this water\u2026).</p> <p>Even though all of this metadata is originating from the manufacturer it\u2019s likely that it will be in different .json blob.  The schema of the .json will typically be determined by the use case, common schemas will be needed for interoperability.  If I want to do a source of origin use case for all bottled water, I will need to get some alignment on what data is required for source of origin and in what format am I expecting it.  We cannot fool ourselves that there will ever be a universal data standard for anything, try as we might.</p> <p>This is where the Machine Learning comes into play.  For any given use case there are a handful of data standards defined by one or more standards bodies.  Those templates can be fed to the algorithm.  Then for each use case specific metadata blob signed to a token that you have a claim on (read that sentence carefully) the algorithm can determine how closely the metadata fits into a known standard. If we switch from ML to AI the algorithm might even be able to do the mapping and fill in any gaps.  At a minimum it should work to catch any abnormalities, at best it should give you real-time feedback on how well your metadata tracks with similar metadata sets.</p> <p>There are two places that you can run this particular scan - the first being prior to signing the metadata, in which case you can include some level of certification into your signature that the metadata meets some set of requirements or standard (you could even have an audit firm do this for you #newbusinessopportuny).  Or you could profile the data after it has been posted, this is the more likely approach when you are receiving a product token that already has metadata signed to it by another party and you want some level of verification as to the legitimacy of the data; remember that signed data is always attributable back to the signing address, but the the actual data itself can be what ever the signer wants it to be (though once it is sighed we can always tell if it is changed after the fact.)</p> <p>One last rabbit's hole to jump down here is the possibility of using a zero knowledge proof to profile the metadata before signature - while this is a really intriguing concept it will carry a certain amount of overhead since generating a proof is computationally heavy, and it may provide limited benefit since the metadata is maintained off chain. But the basic premise goes:</p> <p>1) write a zk circuit that defines the metadata schema including the field names and data types. 2) add any additional data logic rules that might apply on a per element basis (I.e., if a quantity is entered a UOM is required, or only values within a range) 3) Sign the metadata, and ost the roof on-chain making it easy for a third party to verify without actually looking at the data. An alternative to this is to simply register a schema via EAS and reference that you leveraged that schema in your metadata, though here you loose the data verification checks.  Currently attesting under EAS exposes the data but perhaps doing that under privacy, or in conjunction with a 3rd party will be in the future.</p>","tags":["tokenization","supply chain","machine learning"]},{"location":"2023/06/15/wallets-vs-token-metadata/","title":"Wallets vs. Token Metadata","text":"<p>With product tokens we generate a bread crumb trail of metadata that is specific to a given product.  In combination with the minting, transferring, and burning of claims on that token the metadata rounds out the full record of the products lineage and data.</p> <p>The way it works is an individual (a person or an entities) who has a claim on a token (I.e., the token is currently mapped to their wallet address in the latest state of the network) can post a transaction referencing the token id and typically including a CID or some other pointer in the transactions URL.  The CID is a hash of the content of the metadata set and (if using IPFS) is also used to locate the metadata.  Posting the hash on-chain is one way that we later prove the data was not changed.</p> <p>Currently we think of metadata sets by use case, for example - I want to produce a \u201cdigital product passport\u201d so i need the metadata to contain certain data elements required within the DPP.  If I want to use the token as collateral for a loan I need to include certain metadata that details the attributes necessary to assess the value of the asset. There may even be several different metadata sets signed by the by the same wallet to against the same token at the same time to support multiple use cases.</p> <p>Question</p> <p>Maybe we need a universal identifier at the top of a metadata set indicating what schema it is following in support of which use case - kind of like a multihash</p> <p>But sometimes we need to return information about the wallet that is holding the product token, and not just the product token.  For example if I am a manufacturer minting tokens for each product I produce and my wallet is registered to the physical establishment of my manufacturing plant.  There may be additional information about that establishment that I want refrenceable by each token it produces - such as address or facility name.  I may even hold a credential in my wallet stating that this establishment of licensed to produce this product for a given time period.</p> <p>In these cases it would not make sense to repost wallet level metadata each time a new set of product level metadata is signed. It would be better to simply be able to retrieve the wallet level metadata when querying the product token.  So the way this would work for say a digital product passport is that the end consumer would use TCDA to access the metadata about a given token for a given transaction.  It would also look to see if there were wallet level metadata that was applicable at the time of the transaction and return that as well.  </p> <p>Of course this adds a layer that f complexity in that the wallet level data is relevant to product level data only at a specific point in time.  If the wallet level data is updated there needs to be some version control to roll it back.</p> <p>But this approach unlocks another capability through the use of verified credentials.  We want the ability to have a VC issued to a wallet by a trusted institution that qualifies the wallet to do something.  This implies that when an end consumer is accessing metadata using TCDA they would be able to return the VC data as well - so having wallet level data (maybe held on a VC of sorts) is not really any additional work and carries with it the efficiency on only posting wallet level data once.</p> <p>Can we place a soul bound nft in a wallet that has establishment level  - could this be a credential?  This would remove a ton of duplicate metadata from the token.  If you change wallets later the old tokens would be mapped to the old wallet. Data availability might be an issue.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/","title":"Why hasn\u2019t blockchain solved our supply chain issues yet?","text":"<p>This is a drum I\u2019ve been beating for many years now - In my opinion using decentralized networks to create trusted and controlled visibility into the supply chains that literally affect the lives of billions of people every day is inevitable; but we are not there yet\u2026. In this post I summarize my latest thinking on the topic.</p> <p>It\u2019s hard to believe it was almost 4 years ago when COVID-19 was declared a pandemic by the World Health Organization.  I can remember hearing the horrible news stories about how despite the heroic efforts of our healthcare providers thousands of people were dying because they couldn\u2019t get access to life saving medicines and medical devices - the hospitals didn\u2019t even have enough personal protective equipment to keep our doctors and nurses safe.  The reality of an inadequate supply chain was a major contributor the largest public health crisis of our generation.  It affected all of us - I\u2019m pretty sure my family is still using the toilet paper we stock piled during the pandemic\u2026</p> <p>COVID put a spotlight on a systemic pervasive issue with our pharmaceutical supply chain - but the issue is far from new.  Back in 2013 the US FDA passed the Drug Supply Chain Safety Act (DSCSA) which put into law requirements around track-and-trace; given the rapid \u201cdigitization\u201d of supply chain this the mandate should have been pretty easy - it\u2019s just a matter of data standards and interfaces right? 10 years later only portions of DSCSA are even enforceable...  This is not due to lack of effort! Tremendous strides have been made in untangling a decades old, complex, multi-tiered supply chain.  It\u2019s just that these attempts have fallen short.</p> <p>I will admit that this post is a bit lengthy and opinionated, but I think that in order to appreciate the magnitude of change that is needed we have to dig a bit deeper into root cause.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#addressing-the-problem-how-supply-chains-got-this-way","title":"Addressing the problem: how supply chains got this way\u2026","text":"<p>The public health issues due to \u201clack of transparency\u201d in pharmaceutical supply chain have been documented and discussed at length in recent years. The fact that many government agencies are still citing \u201cSupply chain visibility, monitoring, and data sharing processes and platforms\u201d as a topic that requires research and innovation is conviction enough that current approaches are not working.  To be clear, it is not for lack of approaches, many software platforms solve this issue quite well, it is lack of adoption that prevents them from scaling; but I would argue that lack of adoption is driven by poor approach.\u00a0 Any solution that is counting on pressure from business partners or regulators to drive it\u2019s acceptance is approaching the issue from a point of compliance and is incentivizing participation via the \u201cstick\u201d vs. the \u201ccarrot\u201d- they will never be sustainable.</p> <p>For the purposes of this post I would rather frame the question of \u201cwhy did it get like this\u201d as opposed to \u201chow bad is the issue\u201d, because in understanding the why we can perhaps identify the core changes that are needed to correct the problem.  This will result (I hope) in a series of foundational methods/techniques that work well all of the time (\u201dalways get the little things right\u201d) and transcend any particular system or platform.</p> <p>So here we go from the ground up\u2026 fundamentally supply chain is the exchange of assets in return for payment. As complex as our supply chains can be today, they all revolve around facilitating this basic transaction.\u00a0 The \u201cchain\u201d part of supply chain implies that it is a series of transactions that are required to produce and distribute a product.\u00a0 At the edges of a supply chain the exchange of the physical good and payment can take place simultaneously.</p> <p>Think about a farmer selling a harvested wheat crop to a granary on one end of the supply supply spectrum and a hungry person buying a loaf of bread at a bakery - and lets assume these are both cash transactions paid on the spot with no returns for simplicity sake.  This is a simple example of settlement - meaning that one good (wheat or bread) was exchanged for another (money) and there are no residual commitments; the transaction was settled, we are both in agreement with the outcome, lets move on\u2026 \u00a0But as we work our way either downstream or upstream from the ends of the supply chain the quantities of product and the aggregate amounts paid for them begin to increase dramatically (the value of all the grain in the granary is far greater than the value of the individual harvest - the value of all the flour at the bakery is much higher than the price of one loaf of bread) and this aggregation / disaggregation gives rise to a split in the fundamental transaction.  </p> <p>If the granary is able to sell their entire stock to a miller it is very unlikely they will be paid the moment the grain leaves their silos, the miller will may only want to pay upon receipt and inspection of the grain; and even then they may ask for a delay in payment (I had a client once who demanded 120 day payment terms!).  As soon as this happens the exchange of the product and the payment are no longer asynchronous \u2013 there is a separation between the financial ownership and the physical custody.  This results in two interrelated, but separate transactions that require settlement, and this seemingly small rift allows for a vast complexity of buying and selling arrangements.  This should be intuitive to the reader - there is noting new in this centuries old process, but what is interesting is how we evolved systems to address this.  Lynn Aldren\u2019s recent book \u201cBroken Money\u201d does a fantastic job of explaining the monetary side of this evolution.</p> <p>But on the supply chain side we should look more closely at the computer systems in manufacturing enterprises.  They were used for financial accounting long before they were used for managing inventory.\u00a0 As manufacturers required more supplies, they developed procurement processes to facilitate purchasing and to manage their costs.\u00a0 On the other side of the house as customers purchased more products, they developed sales processes to manage their revenues - the monetary system was (and still is) the primary driver of their development.\u00a0 Inventory management was (and mostly still is) an input or output of procurement and sales.\u00a0 ERPs developed to bridge this gap (Enterprise Resource Planning where the resource is many forms of capital - materials, human, knowledge, money, etc) to create efficiencies within the individual organization, but not across organizations.  Granted if the organization is big enough it may seem like the ERP is controlling an entire supply chain, but it is always internally focused in the end - the jurisdiction of the ERP stops at the boundaries of the enterprise.  But supply chain by definition is bigger than any one enterprise so to make this process more efficient they developed ways to connect their procurement and sales systems with their suppliers and customers.  If those supplier and customers happen to be running the same ERP software this is a bit easier (much to the software providers benefit), and although the proliferation of standard Application Programming Interfaces (APIs) and data standards have made things much better as well, it is still complex and clunky - and in some cases completely broken.  Yet these procurement and sales based interfaces are the best we have and we rely on them today to connect our incredibly complex supply chains.</p> <p>All of that is to say that, interfaces between trading partners exist for the primary purpose of settling payments, the transfer of physical inventory is not usually settled, it is just an input or an output (IMO this isn\u2019t a bad thing - a single ERP or system can never/should never govern an entire supply chain).</p> <p>In addition to the maintenance, complexity, and reconciliation challenges inherent in this web of interfaces (I like the term API spaghetti) there is another nuanced issue.\u00a0 Payments are fungible where inventory is not.\u00a0 If company A pays company B $6 US dollars for a product, that $6 is (eventually) debited from company A and credited to company B.\u00a0 There is no requirement (or reason) to determine if company B used that same $6, or a fraction of it, to buy something from company C.\u00a0 But this is not the case for inventory, even if the individual units of inventory themselves are interchangeable (i.e., one screw in a box of 100) they are identifiable at some level \u2013 unit, lot/batch, product, etc.).\u00a0 So, to transfer this type of knowledge between trading partners the EDI 867 Product Transfer and Resale Report was designed explicitly to pass along product information.  And there is a bit of handshake between the trading partners if they use a corresponding EDI 997 Functional Acknowledgment to show the EDI was received and in good format - but that receipt does not guarantee that the inventory was moved out of one system and into another we still have to rely on a separate system of checks and controls to keep the information in the digital records in sync with the physical movements of inventory.\u00a0 This manifests itself in the form of something like this:</p> <p>Seller: \u201cUpon shipment of x units of product y in relation to order z, I will debit x units of product y from my inventory system\u201d Buyer: \u201cUpon receipt of x units of product y in relation to order z, I will credit x units of product y to my inventory system (in which I may or may not refer to this product as product y)</p> <p>These statements are only made for the sake of the buyers\u2019 and sellers\u2019 internal inventory management systems which they use for their internal planning.  They are only as strong as their internal processes and controls - they are not settlements. Further, just as with the payment, the history of transfers related to that particular product is not inherently tied to the product record, it is buried in the paper trail of the procurement process - and that paper trail usually breaks each time the product is transferred.</p> <p>The core issue at the bottom of all of this is the lack of a way to \u201csettle inventory transfers\u201d and a way to \u201clog and share a history of those transfers\u201d between trading partners.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#past-and-current-attempts-to-solve-and-why-they-arent-working","title":"Past and current attempts to solve (and why they aren\u2019t working)","text":"<p>Of course, the above description is over generalized and as I mentioned these are not new issues; there are many solutions either available or under development that aim to address inventory settlement - often rolled up into solutions for \u201csupply chain visibility\u201d or \u201ctrack-and-trace\u201d in response to the current description of the problem.  </p> <p>Lets also establish up front the fact that inventory settlements between training partners are confidential transactions.  Meaning that anyone not involved in the transaction should not be able to infer any information about it.  It might be OK for it to be known that two entities are trading partners (in pharma we have the concept of Authorized Trading Partners already established) but which products, how many, and how often are trade secrets.  If a competitor were able to get that information there would be many opportunities to arbitrage the supply chain.</p> <p>I find that all current solutions fall into one of 3 categories:</p> <p>1) Direct peer to peer connections: These are interfaces that arose from procurement processes and are built and maintained by trading partners that pass information about their product from one system to another through some mutually agreed upon protocol and data standard. These have benefitted greatly over time through better use of Application Programming Interfaces (APIs) and data standards that have developed out of necessity from organizations like IEEE and GS1, as well as the EDI standards referenced above. In a multi-tiered supply chain, many peer-to-peer interfaces will need to be \u201cstitched together\u201d to form the complete linage of a given product.</p> <p>Primary Benefits:</p> <ul> <li>Only requires the coordination of two parties (the buyer and the seller) for any one interface.</li> <li>The information exchanged between them is relatively easy to secure (i.e., their transactions are confidential)</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Difficult to scale, a buyer or seller may need to develop and support many discrete interfaces.</li> <li>Multiple interfaces often result in significant master data management issues and requires reconciliations and mappings.</li> <li>Product data does not \u201cinherently pull-through\u201d when the same product is exchanged multiple times.</li> </ul> <p>2) Private/consortia-controlled platforms or networks: A centralized solution that serves as a coordinator, and in some cases a governor for the supply chain. Often referred to as a \u201ccontrol tower\u201d these solutions work well when trading partners can agree upon a common trusted intermediary.</p> <p>Primary Benefits:</p> <ul> <li>Creates a high level of regulatory control over certain industries.</li> <li>Transactions can be kept confidential between trading partners and the trusted intermediary.</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Often only provide value when at scale and can be very difficult/expensive to establish.</li> <li>Inevitably lead to the centralization of risk and control and are inherently bureaucratic.</li> <li>Typically focus on solving one pervasive industry issue \u2013 not generally composable with other solutions/business models which lead to limited business value.</li> <li>For large multinational organizations or for industries with many small participants and a few large ones these create a barrier to entry.</li> </ul> <p>3) Public networks: Attempts to leverage large open networks (primarily blockchains) for supply chain use cases. It is necessary to leverage networks that support code/rule execution within the network (e.g., smart contracts)</p> <p>Primary Benefits:</p> <ul> <li>Can leverage blockchains for what they are good for (global coordination and management of digital scarcity)</li> <li>The \u201ccommon ledger\u201d eliminates reconciliation issues between systems and the transaction history lets the data \u201cpull through\u201d with a high level of integrity.</li> <li>Unique tokens can be used to represent products and track simultaneous claims on them (i.e., ownership vs. custody)</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Public networks are, well, public\u2026 enforcing the confidential nature of transactions as described above often leads to solutions that are built on top of the public network, and those solutions fall into category 1 or 2.</li> <li>By their nature the smart contracts (and their business logic within) that would govern supply chain use cases are also public.</li> </ul> <p>There is yet another issue inherent to our current state that is often pervasive across all three of the categories above:\u00a0 Because we don\u2019t actually settle inventory transactions most solutions attempt to recreate the paper trail of our current or legacy supply chain systems.\u00a0 In this way they leverage the type of data that is already exchanged in a peer-to-peer interface (this is often the easiest source of data to tap into).\u00a0 As discussed, those systems are designed around the procurement process and digitize documents such as Sales Orders, Purchase Orders, Advanced Shipping Notifications, Bill of Ladings, Goods Receipts, and Invoices; each of these documents make a reference to the product and quantity being transferred (among other procurement data such as price and payment terms), but not all of them represent specific claims on the actual product.\u00a0 </p> <p>In the pharmaceutical supply chain, an additional level of detail can be included in the documents which lists the specific serial numbers that are being transferred.\u00a0 But even if the data in the documents are 100% accurate because the buyer and the seller operate on two different systems this doesn\u2019t equate to settlement - again, there is no programmatic guarantee that the inventory is debited from one system and credited to another; while this might be achievable between a small ecosystem of trading partners it is impossible to achieve at scale without leveraging category 2 or category 3 solutions above because trading partners manage inventory on separate books.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#a-new-foundational-approach-to-settle-inventory-transactions-is-needed","title":"A new foundational approach to settle inventory transactions is needed","text":"<p>If instead of focusing on \u201crecreating the procurement paper trail\u201d in a digital fashion we were to focus on a way to fundamentally settle inventory transactions we might be able to build a better mouse trap.  </p> <p>This starts by thinking about the relationship between supply chain participants (I.e., one side of the trading partners used throughout this post), their products (the asset), and events that occurs at a point in time - the who, what, when (where, why and how are also very important but not as pertinent to the settlement - EPCIS 2.0 does a great job explaining these factors).  That relationship can be described as a \u201cclaim\u201d and for any given product there may be multiple claims held my multiple participants and any point in time.</p> <p>The most common example of claims are ownership and custody.  If your friend borrows your car you are still the owner, but your friend has custody.  Same car, different claim claims.  Further if you happen to lease the car from a bank, then the bank has a claim on the car as well, in the form of a lien.  As the owner you have the right to sell your car, but the bank can impair your right to sell if you don\u2019t clear the lien first, just like your friend can impair your ability to sell if they don\u2019t give it back (or if they wreck it\u2026). There are several different types of claims that describe the rights a participant has on a product, and it is not a standard set - \u201cclaims\u201d are a construct that can be defined per product class as and have \u201cclaim rules\u201d that describe how those claims effect one another.</p> <p>Settling an inventory transaction is in effect settling a claim - or rather creating a shared state that maps the claims to participants for a given product at a point in time.  Changes to this state over time in the form of a claim-state log become the history of who, what, and when.  If we also add in the ability for a claim holder to attest to a piece of information about a product then we can round out the where, why, and how components. Basic rules like \u201ca claim for a given product can only be held by one participant at a time\u201d or \u201cyou must hold a claim in order to transfer it\u201d can enforce the settlement.  </p> <p>The last component that is needed is a way to traverse the claims-state log forward and backward for a given product.  This would give us the \u201ctrack-and-trace\u201d components that we have been seeking, and if we were able to limit that ability to only those participants who hold (or have held in the past) claims on that product we would preserve the confidential nature of the supply chain.  A technique like this would be a method for Confidential Transactions with Embedded Rights (CSER) and is a public good that Zeroth Technology is looking to create for the world.</p>","tags":["tokenization","supply chain"]},{"location":"archive/2024/","title":"May 2024","text":""},{"location":"archive/2023/","title":"November 2023","text":""},{"location":"category/fundamentals/","title":"Fundamentals","text":""},{"location":"category/supply-chain/","title":"Supply Chain","text":""},{"location":"category/attestation/","title":"Attestation","text":""},{"location":"tags/","title":"Tags","text":"<p>return to index</p> <p>We add tags to help group our writings.  You can find a list of them here:</p>"},{"location":"tags/#attested-sensors","title":"attested sensors","text":"<ul> <li>Attested Sensors &amp; Source Data Synthesis Under Zero Knowledge</li> </ul>"},{"location":"tags/#autonomous-worlds","title":"autonomous worlds","text":"<ul> <li>Interobjectivity... ?</li> </ul>"},{"location":"tags/#machine-learning","title":"machine learning","text":"<ul> <li>Product Token Metadata Profiling</li> </ul>"},{"location":"tags/#supply-chain","title":"supply chain","text":"<ul> <li>Digital Twin \u2260 Product Token</li> <li>Does Knowing Make You Accountable?</li> <li>Interobjectivity... ?</li> <li>Product-Oriented Data Structure</li> <li>Product Token Metadata Profiling</li> <li>Wallets vs. Token Metadata</li> <li>Why hasn\u2019t blockchain solved our supply chain issues yet?</li> </ul>"},{"location":"tags/#tokenization","title":"tokenization","text":"<ul> <li>Digital Twin \u2260 Product Token</li> <li>Does Knowing Make You Accountable?</li> <li>Product-Oriented Data Structure</li> <li>Product Token Metadata Profiling</li> <li>Wallets vs. Token Metadata</li> <li>Why hasn\u2019t blockchain solved our supply chain issues yet?</li> </ul>"}]}